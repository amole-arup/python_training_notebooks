{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy and Pandas ![NumPy logo](Resources\\Images\\numpy_logo.png) ![Pandas logo](Resources\\Images\\pandas_logo.png) \n",
    "These are specialised libraries for handling large arrays of data.\n",
    "* [NumPy](http://www.numpy.org/) is *\"the fundamental package for scientific computing with Python\"* - it implements fast arrays and matrix calculations. [Documentation](https://docs.scipy.org/doc/).\n",
    "* [Pandas](https://pandas.pydata.org/) is *\"an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language\"* - it adds features for data science (named columns and rows). [Documentation](http://pandas.pydata.org/pandas-docs/stable/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy ![NumPy logo](Resources\\Images\\numpy_logo.png)\n",
    "\n",
    "From the documentation:\n",
    "> NumPy contains among other things:\n",
    "> \n",
    "> 1. a powerful N-dimensional array object\n",
    "2. sophisticated (broadcasting) functions\n",
    "3. tools for integrating C/C++ and Fortran code\n",
    "4. useful linear algebra, Fourier transform, and random number capabilities\n",
    "> \n",
    "> Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.\n",
    "\n",
    "The NumPy arrays function in a similar way to If you are already familiar with Matlab, then you will find a comparison of rough Matlab equivalents in the [online guide for Matlab users](https://docs.scipy.org/doc/numpy-1.15.0/user/numpy-for-matlab-users.html).\n",
    "\n",
    "We will focus on the use of NumPy to handle large arrays of data.\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "* [Loading CSV into 2D matrices](https://stackoverflow.com/questions/4315506/load-csv-into-2d-matrix-with-numpy-for-plotting)\n",
    "* [Structured arrays in SciPy](https://docs.scipy.org/doc/numpy/user/basics.rec.html)\n",
    "* [Vectorisation in place of for-loops](https://towardsdatascience.com/why-you-should-forget-for-loop-for-data-science-code-and-embrace-vectorization-696632622d5f)\n",
    "* [NumPy array programming](https://realpython.com/numpy-array-programming/)\n",
    "* [Computation on Arrays using ufuncs](https://jakevdp.github.io/PythonDataScienceHandbook/02.03-computation-on-arrays-ufuncs.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Arrays\n",
    "NumPy Arrays (ndarray) have some key features:\n",
    "1. They are built for fast array and matrix operations, especially for large arrays. \n",
    "2. They are homogeneous and strongly typed - you need to decide what the data type is, and that is the only type that it can contain - i.e. all integers or all floats or all strings\n",
    "3. It is best to define the dimensions of the array at the outset - appending to arrays is relatively slow\n",
    "4. Many matrix operations are defined and they are fast\n",
    "5. NumPy arrays are used by other libraries, such as SciPy, pandas, tensorflow and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will create a 1D array filled with zeros\n",
    "import numpy as np\n",
    "\n",
    "x = np.zeros(4, dtype=int)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create an array of any dimension\n",
    "y = np.ones((2,3,4), dtype = float)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can identify the dimensions of the array by referencing the `shape` property\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_arr_1 = np.arange(0, 6).reshape(3, 2)\n",
    "np_arr_2 = np.arange(0, 8).reshape(2, 4)\n",
    "\n",
    "print('First array')\n",
    "print(np_arr_1)\n",
    "print('Second array')\n",
    "print(np_arr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two types of array multiplication can be carried out using the `*` operator (element-wise multiplication) and the `@` operator (matrix multiplication).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise multiplication\n",
    "my_result_arr = np_arr_1 * np_arr_1\n",
    "print('First array x First array (element-wise)')\n",
    "print(my_result_arr)\n",
    "np_arr_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Multiplication\n",
    "my_result_arr = np_arr_1 @ np_arr_2\n",
    "print('Matrix shapes:', np_arr_1.shape, np_arr_2.shape, my_result_arr.shape)\n",
    "print('First array x Second array')\n",
    "print(my_result_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication can also carried out using the `np.matmul` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(np_arr_1, np_arr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Pure Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can convert NumPy arrays into simple lists\n",
    "list_1 = np_arr_1.tolist()\n",
    "list_2 = np_arr_2.tolist()\n",
    "list_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication using Simple Python For-loops\n",
    "Note that the `@` operator cannot be used on pure python list that have the arrangement of arrays / matrices, so we have to create a new function.  Indeed, neither can the standard operators (`+`, `-`, `*`, `/` etc) be used for element-wise operations on pure python lists acting as arrays or as matrices - i.e. `list_1 * list_2` will not work either. In this case list comprehension provides a quick solution - `[x * y for x, y in zip(list_1, list_2)]`for 1D array, and `[[x * y for x, y in zip(a, b)] for a, b in zip(list_1, list_1)]` for a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Multiplication \n",
    "def mmult(list_1, list_2):\n",
    "    my_result_arr = [[0 for col in range(len(list_2[0]))] for row in range(len(list_1))]\n",
    "    # iterate through rows of list_1\n",
    "    for i in range(len(list_1)):\n",
    "       # iterate through columns of list_2\n",
    "       for j in range(len(list_2[0])):\n",
    "           # iterate through rows of list_2\n",
    "           for k in range(len(list_2)):\n",
    "               my_result_arr[i][j] += list_1[i][k] * list_2[k][j]\n",
    "    return my_result_arr\n",
    "\n",
    "\n",
    "print('For-loop Matrix Multiplication')\n",
    "mmult(list_1, list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication using List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List Comprehension')\n",
    "[[sum(a*b for a,b in zip(X_row,Y_col)) for Y_col in zip(*list_2)] for X_row in list_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing Comparison - Small Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For-loop Multiplication  : ', end=\"\")\n",
    "%timeit mmult(list_1, list_2)\n",
    "print('List Comp Multiplication : ', end=\"\")\n",
    "%timeit [[sum(a*b for a,b in zip(X_row,Y_col)) for Y_col in zip(*list_2)] for X_row in list_1]\n",
    "print('NumPy Multiplication     : ', end=\"\")\n",
    "%timeit np.matmul(np_arr_1, np_arr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NumPy can be significantly faster, but is not always a lot faster for operations on short lists or small arrays. List comprehension is almost always faster than for-loops, but by less than an order of magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing Comparison - Large Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building large arrays\n",
    "np_arr_3 = np.arange(0, 60000).reshape(300, 200)\n",
    "np_arr_4 = np.arange(0, 80000).reshape(200, 400)\n",
    "list_3 = np_arr_3.tolist()\n",
    "list_4 = np_arr_4.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For-loop Multiplication  : ', end=\"\")\n",
    "%timeit mmult(list_3, list_4)\n",
    "print('List Comp Multiplication : ', end=\"\")\n",
    "%timeit [[sum(a*b for a,b in zip(X_row,Y_col)) for Y_col in zip(*list_4)] for X_row in list_3]\n",
    "print('NumPy Multiplication     : ', end=\"\")\n",
    "%timeit np.matmul(np_arr_3, np_arr_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NumPy is significantly faster than pure Python in this case.\n",
    "* ~1.6x faster with list comprehensions\n",
    "* ~500x faster with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Structured Arrays\n",
    "NumPy arrays are fast, but not very flexible and they do not carry metadata that describes the data they contain. In addition to the simple ndarrays, NumPy also provides structured arrays that allow you to mix different types of data (although each column has to be of the same type) and they can also carry descriptions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Arrays\n",
    "steel_mat = np.array([('S275', 275, 200000), ('S355', 355, 200000), ('S460', 460, 200000)], \n",
    "             dtype=[('name', 'U10'), ('fy', 'f4'), ('E_mod', 'f4')])\n",
    "steel_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to address the data by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steel_mat['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas ![Pandas logo](Resources\\Images\\pandas_logo.png)\n",
    "\n",
    "Pandas is a popular tool in data science. It provides a clean interface to structured data and also many tools for cleaning and processing this data. It is fast because it is based on NumPy. \n",
    "\n",
    "The key data types are `Series`, `Indexes` and `DataFrames` ([a summary is provided here](https://pandas.pydata.org/pandas-docs/stable/dsintro.html)). These are based on underlying data types such as integers and floats ([there is a summary here](http://pbpython.com/pandas_dtypes.html)). Pandas dataframes are based on the dataframes created for the R-language ([R is a popular and powerful statistical programming language](https://cran.r-project.org/) - [data frame documentation here](https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/data.frame)).\n",
    "\n",
    "The following website provides a useful introduction to Pandas.\n",
    "* [10 Minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "\n",
    "There is also a Jupyter-based tutorial called [Pandas Cookbook](http://pandas.pydata.org/pandas-docs/stable/tutorials.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Series and DataFrames \n",
    "Series and DataFrames can be created directly and DataFrames can also be created from Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.Series([1,3,5,np.nan,8]) # note that while all data has to be of the same data type, some can be 'Not a Number' (NaN)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide metadata for each data element (index) and also for the whole series (name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series([1,3,5,np.nan,8], name = 'Animals', index = ('Mouse','Rat','Camel','Bat','Zebra'))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a pair of series and then combine them into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(['Lily', 'MingChun', 'Mary', 'Chris','Lee'], name = 'Name') \n",
    "s2 = pd.Series([3088, 3142, 5514, 2221, 3001], name = 'Number', dtype = 'int') \n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be combined using the concatenation command (`concat`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([s1, s2], axis=1)\n",
    "print(df.index)\n",
    "print(df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is then possible to access elements of the dataframe using the column names\n",
    "df['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If the name is simple then it can also be used to access the column directly\n",
    "df.Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Python lists\n",
    "Pandas can directly import Python collections, lists - tuples and dictionaries. It carries out intelligent interpretation of the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_list = [[5,8],[10,3],[6,1]]\n",
    "py_dict = [{ 'species' : 'pangolin', 'class': 'mammal' },\n",
    "           { 'species' : 'gecko', 'class': 'lizard' },\n",
    "           { 'species' : 'mantis', 'class': 'insect' },\n",
    "           { 'species' : 'echidna', 'class': 'mammal' },\n",
    "           { 'species' : 'bulbul', 'class': 'bird' }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(py_list, columns = ['A', 'B'], index = [11,12,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(py_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data into Pandas\n",
    "In addition to direct importing of Python lists, Pandas has a number of tools for importing (and exporting) data ([full listing here](https://pandas.pydata.org/pandas-docs/stable/io.html)):\n",
    "1. pandas.read_csv\n",
    "2. pandas.read_excel\n",
    "3. pandas.read_json\n",
    "4. pandas.read_sql\n",
    "5. pandas.read_clipboard\n",
    "6. ... and others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_data_df = pd.read_csv('data7.csv')\n",
    "pd_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print out information on the meta-data\n",
    "print('Indexes:', pd_data_df.index)\n",
    "print('Columns:', pd_data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the individual columns using the column names \n",
    "# (NB head() limits the number of lines)\n",
    "pd_data_df['sin_x'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the header is simple text, then we can also access it using a simple reference\n",
    "pd_data_df.sin_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can identify which of the elements in a column meet a certain criterion \n",
    "pd_data_df['sin_x'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then use this pattern (mask) to filter the data in the full table\n",
    "pd_data_df[pd_data_df['sin_x'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying functions to pandas DataFrames\n",
    "One of the strengths of pandas dataframes is that you can carry out NumPy vectorised operations. For example, you can apply the NumPy `log10` operation to all the data in the dataframe. In the following example, we are extracting the first six rows from the dataframe above (using the `[]` notation) and are applying the NumPy function. \n",
    "\n",
    "Note that this will generate some errors - log10 of zero is infinity, and log10 of negative numbers is not valid (and results in a 'Not a Number' (`NaN`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(pd_data_df[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "***Note: this section is not covered by the video***\n",
    "\n",
    "One of the standard problems with imported data is that it can include missing or irregular data (mis-typed or incorrectly configured). Cleaning up the data can be very time-consuming. NumPy and Pandas include tools to assist with this. On importing, bad data will either be replaced by an `NaN` data object (**n**ot **a** **n**umber), or will be present as inconsistent data.\n",
    "\n",
    "This is a large topic. Some guidance on data cleaning is available at these web-sites:\n",
    "* [Pandas Cookbook: Chapter 7: Cleaning data](https://nbviewer.jupyter.org/github/jvns/pandas-cookbook/blob/v0.2/cookbook/Chapter%207%20-%20Cleaning%20up%20messy%20data.ipynb)\n",
    "* https://www.dataoptimal.com/data-cleaning-with-python-2018/\n",
    "* https://realpython.com/python-data-cleaning-numpy-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from a list of lists containing building data.\n",
    "# Note that the data is 'dirty' because some data is missing and because Storeys and GFA \n",
    "# are recorded in inconsistent formats\n",
    "\n",
    "import numpy as np  # imported so that this cell can be run on its own...\n",
    "import pandas as pd\n",
    "dirty_data = [['Building_Name','GFA','Storeys','Zipcode'], ['Richland Bldg', '20000', 5, 98105],\n",
    "              ['Prosper Court', '15,000', 4, '--'], ['Bumper Tower', '55,000sf', 12, '10045'],\n",
    "              ['MegaMall', '214,000', '3F 1B' , ]]\n",
    "# import into DataFrame and use first line as column titles (using list slicing)\n",
    "dirty_df = pd.DataFrame(dirty_data[1:], columns = dirty_data[0]) \n",
    "dirty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - cleaning up the GFA column\n",
    "# We can clean up the GFA column by doing a string replace using a regex pattern that matches every  \n",
    "# character that is not either a number or a dot - [0-9.] - the '^' reverses the selection\n",
    "# We can also convert the data from strings to floats using the 'astype' function\n",
    "dirty_df.GFA.str.replace('[^0-9.]', '', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can create a cleaner dataframe where we replace the data in the dataframe\n",
    "cleaner_df = dirty_df.copy()\n",
    "cleaner_df.GFA = dirty_df.GFA.str.replace('[^0-9.]', '', regex=True).astype(float)\n",
    "cleaner_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying User-Defined Functions to a DataFrame\n",
    "It is possible to apply user-defined functions to a DataFrame. In the following example we will take the first and last column and average the values row-by-row. This is a vectorised calculation and runs very quickly.\n",
    "\n",
    "Functions can be defined in the normal way and can be referenced by name, or then can be included in the operation as an un-named function (a lambda function). The lambda function below is exactly the same as the function `my_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a):\n",
    "    \"\"\"Average second and last element of a 1-D array\"\"\"\n",
    "    return (a[1] + a[-1]) / 2\n",
    "    #print(a[1], a[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd_data_df.apply(my_func, axis = 1)\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions can also be defined using the `lambda` format. This is good for simple calculations, especially as it is more compact.\n",
    "```python\n",
    "lambda a: (a[1] + a[-1]) / 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_df = pd_data_df.apply(lambda a: (a[1] + a[-1]) / 2, axis = 1)\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can add this to the original dataframe by assigning it to a new column name\n",
    "pd_data_df['Ave'] = pd_data_df.apply(my_func, axis = 1)\n",
    "pd_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organising data using *DataFrame.groupby*\n",
    "***Note: this section is not covered by the video***\n",
    "\n",
    "One powerful function in Pandas is the ability to group data by one of the columns. This effectively treats the data as multidimensional. In the following example we will group the data and then summarise the data using the `sum` function.\n",
    "\n",
    "Additional information is available:\n",
    "* http://pandas.pydata.org/pandas-docs/stable/groupby.html\n",
    "* [Pandas Cookbook: Ch 4: Groupby / Aggregate](https://nbviewer.jupyter.org/github/jvns/pandas-cookbook/blob/v0.2/cookbook/Chapter%204%20-%20Find%20out%20on%20which%20weekday%20people%20bike%20the%20most%20with%20groupby%20and%20aggregate.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe from a list of lists containing building (clean) data:\n",
    "building_data = [['Building_Name', 'Type', 'GFA', 'Storeys', 'Zipcode'], ['Richland Bldg', 'Residential', 20000., 5, 98105],\n",
    "              ['Prosper Court', 'Residential', 15000., 4, 98105], ['Bumper Tower', 'Commercial', 55000., 12, 10045],\n",
    "              ['MegaMall', 'Commercial', 214000., 3, 10045], ['Mini-mall', 'Commercial', 75000., 1, 98105]]\n",
    "df = pd.DataFrame(building_data[1:], columns = building_data[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort into groups according to 'Zipcode' and report the group contents\n",
    "df.groupby(['Zipcode']).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After grouping by Zipcode, calculate the sum of the GFA for each Zipcode.  \n",
    "df.groupby(['Zipcode'])['GFA'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "We have seen how there are libraries that assist in working with large datasets. NumPy can bring significant savings in calculations, especially if calculations are vectorised instead of processed as for-loops.\n",
    "\n",
    "Note that additional time savings may be made in some cases by using the [Numba library](http://numba.pydata.org/) that can not only speed up vectorised formulas, but can also send calculations to multiple processors (parallel processing), but can also make use of GPUs and clusters (such as through the [Dask library](http://docs.dask.org/en/latest/)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Read the data from the accompanying file called `Traffic_Data.txt` (containing tab-separated data) into a pandas dataframe (note that the first line contains the vehicle categories). \n",
    "2. Clean up the missing data\n",
    "3. Create new columns for commercial vehicles (sum of trucks, buses and taxis) and private vehicles (sum of cars and motorcycles)\n",
    "4. Print a bar chart of commercial and private vehicles against the hour.\n",
    "\n",
    "### Reference Material\n",
    "* https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n",
    "* https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.plot.bar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
